<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Technology Feed]]></title><description><![CDATA[Latest Technology articles]]></description><link>https://github.com/iMahir/newrss</link><generator>RSS for Node</generator><lastBuildDate>Mon, 26 Aug 2024 11:36:13 GMT</lastBuildDate><atom:link href="https://github.com/iMahir/newrss/data/rss/technology.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 26 Aug 2024 11:36:13 GMT</pubDate><language><![CDATA[en]]></language><ttl>600000</ttl><item><title><![CDATA[How would we even know if AI went rogue?]]></title><description><![CDATA[
											

						
<figure>

<img alt="An emergency road sign surrounded by orange cones on a deserted strip of highway reads “A.I. ahead.”" data-caption="Congress needs to understand artificial intelligence capabilities better in order to mitigate future risks." data-portal-copyright="" data-has-syndication-rights="1" src="https://platform.vox.com/wp-content/uploads/sites/2/2024/08/GettyImages-1479589931.jpg?quality=90&#038;strip=all&#038;crop=0,0,100,100" />
	<figcaption>Congress needs to understand artificial intelligence capabilities better in order to mitigate future risks.</figcaption>
</figure>
<p class="has-text-align-none">As the frontier of artificial intelligence advances at a breakneck pace, the US government is struggling to keep up. Working on AI policy in Washington, DC, I can tell you that before we can decide how to govern frontier AI systems, we first need to see them clearly. Right now, we’re navigating in a fog.</p>

<p class="has-text-align-none">My role as an AI policy fellow at the Federation of American Scientists (FAS) involves developing bipartisan ideas for improving the government’s ability to analyze current and future systems. In this work, I interact with experts across government, academia, civil society, and the AI industry. What I’ve learned is that there is no broad consensus on how to manage the potential risks of breakthrough AI systems without hampering innovation. However, there is broad agreement that the US government needs <a href="https://arxiv.org/pdf/2108.12427">better information about AI companies’ technologies and practices</a>, and more capacity to respond to both catastrophic and more insidious risks as they arise. Without detailed knowledge of the latest AI capabilities, policymakers can’t effectively assess whether current regulations are sufficient to prevent misuses and accidents, or whether companies need to take additional steps to safeguard their systems. </p>

<p class="has-text-align-none">When it comes to <a href="https://www.nrc.gov/reading-rm/doc-collections/fact-sheets/oversight.html">nuclear power</a> or <a href="https://www.faa.gov/about/office_org/headquarters_offices/avs">airline safety</a>, the federal government demands timely information from the private companies in those industries to ensure the public’s welfare. We need the same insight into the emerging AI field. Otherwise, this information gap could leave us vulnerable to unforeseen risks to national security or lead to overly restrictive policies that stifle innovation.</p>

<h2 class="wp-block-heading has-text-align-none">Progress in Washington</h2>

<p class="has-text-align-none">Encouragingly, Congress is making gradual progress in improving the government’s ability to understand and respond to novel developments in AI. Since ChatGPT’s debut in late 2022, AI has been taken more seriously by legislators from both parties and both chambers on Capitol Hill. The House formed a <a href="https://fedscoop.com/house-leaders-announce-new-bipartisan-ai-task-force/">bipartisan AI task force</a> with a directive to balance innovation, national security, and safety. Senate Majority Leader Chuck Schumer (D-NY) organized a series of <a href="https://www.techpolicy.press/us-senate-ai-insight-forum-tracker/">AI Insight Forums</a> to collect outside input and build a foundation for AI policy. These events informed the bipartisan Senate AI working group’s <a href="https://www.schumer.senate.gov/imo/media/doc/Roadmap_Electronic1.32pm.pdf">AI Roadmap</a> that outlined areas of consensus, including “development and standardization of risk testing and evaluation methodologies and mechanisms” and an AI-focused Information Sharing and Analysis Center.</p>

<p class="has-text-align-none">Several bills have been introduced that would enhance information sharing about AI and bolster the government’s response capabilities. The Senate’s bipartisan <a href="https://www.congress.gov/bill/118th-congress/senate-bill/3312/text">AI Research, Innovation, and Accountability Act</a> would require companies to submit risk assessments to the Department of Commerce before deploying AI systems that may impact critical infrastructure, criminal justice, or biometric identification. Another bipartisan bill, the <a href="https://www.congress.gov/bill/118th-congress/senate-bill/4769/text/is?overview=closed&amp;format=txt">VET AI Act</a> (which FAS <a href="https://www.hickenlooper.senate.gov/press_releases/hickenlooper-capito-introduce-bipartisan-bill-to-create-guidelines-for-third-party-audits-of-ai/#:~:text=The%20VET%20AI%20Act%20would,in%20compliance%20with%20established%20guardrails.">endorsed</a>), proposes a system for independent evaluators to audit and verify AI companies&#8217; compliance with established guidelines, similar to existing practices in the financial industry. These bills cleared the Senate Commerce committee in July and may receive a floor vote in the Senate before the 2024 election. </p>

<p class="has-text-align-none">There has also been promising progress in other parts of the world. In May, the UK and Korean governments announced that most of the world’s leading AI companies agreed to a new set of <a href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024">voluntary safety commitments</a> at the AI Seoul Summit. These pledges include identifying, assessing, and managing risks associated with developing the most advanced AI models, drawing on companies’ <a href="https://fas.org/publication/scaling-ai-safety/">Responsible Scaling Policies</a> pioneered in the past year that provide a roadmap for future risk mitigation as AI capabilities develop. The AI developers also agreed to provide transparency on their approaches to frontier AI safety, including “sharing more detailed information which cannot be shared publicly with trusted actors, including their respective home governments.”&nbsp;</p>

<p class="has-text-align-none">However, these commitments lack enforcement mechanisms and standardized reporting requirements, making it difficult to assess whether or not companies are adhering to them. </p>

<p class="has-text-align-none">Even some industry leaders have voiced support for increased government oversight. Sam Altman, CEO of OpenAI, <a href="https://time.com/6280372/sam-altman-chatgpt-regulate-ai/">emphasized this point</a> early last year in testimony before Congress, stating, “I think if this technology goes wrong, it can go quite wrong, and we want to be vocal about that. We want to work with the government to prevent that from happening.” Dario Amodei, CEO of Anthropic, has taken that sentiment one step further; after the publication of Anthropic’s <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy</a>, he <a href="https://www.anthropic.com/news/uk-ai-safety-summit">expressed his hope</a> that governments would turn elements from the policy into “well-crafted testing and auditing regimes with accountability and oversight.”</p>

<p class="has-text-align-none">Despite these encouraging signs from Washington and the private sector, significant gaps remain in the US government’s ability to understand and respond to rapid advancements in AI technology. Specifically, three critical areas require immediate attention: protections for independent research on AI safety, early warning systems for AI capabilities improvements, and comprehensive reporting mechanisms for real-world AI incidents. Addressing these gaps is key for protecting national security, fostering innovation, and ensuring that AI development advances the public interest.</p>

<h2 class="wp-block-heading has-text-align-none">A safe harbor for independent AI safety research</h2>

<p class="has-text-align-none">AI companies often discourage or even <a href="https://crfm.stanford.edu/2024/04/08/aups.html">threaten to ban</a> researchers who identify safety flaws from using their products, creating a chilling effect on essential independent research. This leaves the public and policymakers in the dark about possible dangers from widely used AI systems, including threats to US national security. Independent research is vital because it provides an external check on the claims made by AI developers, helping to identify risks or limitations that may not be apparent to the companies themselves.</p>

<p class="has-text-align-none">One significant proposal to address this issue is that companies should offer <a href="https://fas.org/publication/safe-harbor-for-ai-researchers/">legal safe harbor and financial incentives for good-faith AI safety and trustworthiness research</a>. Congress could offer “<a href="https://www.ajl.org/bugs">bug</a>” <a href="https://www.anthropic.com/news/model-safety-bug-bounty">bounties</a> to AI safety researchers who identify vulnerabilities and extend legal protections to experts studying AI platforms, similar to those proposed for social media researchers in the <a href="https://www.congress.gov/bill/118th-congress/senate-bill/1876/text">Platform Accountability and Transparency Act</a>. In an <a href="https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/">open letter</a> earlier this year, over 350 leading researchers and advocates called for companies to give such protections for safety researchers, but <a href="https://openreview.net/forum?id=dLojMSgSFW&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICML.cc%2F2024%2FConference%2FAuthors%23your-submissions)">no company</a> has yet done so.</p>

<p class="has-text-align-none">With these protections and incentives, thousands of American researchers could be empowered to stress-test AI systems, allowing real-time assessments of AI products and systems. The US AI Safety Institute has included similar protections for AI researchers in its draft guidelines on “<a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.800-1.ipd.pdf">Managing Misuse Risk for Dual-Use Foundation Models</a>,” and Congress should consider codifying these best practices.</p>

<h2 class="wp-block-heading has-text-align-none">An early warning system for AI capability improvements</h2>

<p class="has-text-align-none">The US government’s approach to identifying and responding to frontier AI systems’ potentially dangerous capabilities is limited and unlikely to keep pace with new AI capabilities if they continue to rapidly increase. The knowledge gap within the industry leaves policymakers and security agencies unprepared to address emerging AI risks. Worse, the potential consequences of this asymmetry will compound over time as AI systems become both more risky and more widely used. </p>

<p class="has-text-align-none">Establishing an <a href="https://fas.org/publication/an-early-warning-system-for-ai/">AI early warning system</a> would equip the government with the information it needs to get ahead of threats from artificial intelligence. Such a system would create a formalized channel for AI developers, researchers, and other relevant parties to report <a href="https://www.govinfo.gov/content/pkg/FR-2023-11-01/pdf/2023-24283.pdf#page=4">AI capabilities</a> that have both civilian and military applications (such as <a href="https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/">uplift for biological weapons research</a> or <a href="https://apnews.com/article/microsoft-generative-ai-offensive-cyber-operations-3482b8467c81830012a9283fd6b5f529">cyber offense</a>) to the government. The Commerce Department’s Bureau of Industry and Security could serve as an information clearinghouse, receiving, triaging, and forwarding these reports to other relevant agencies.</p>

<p class="has-text-align-none">This proactive approach would provide government stakeholders with up-to-date information about the latest AI capabilities, enabling them to assess whether current regulations are sufficient or whether new safeguards are needed. For instance, if advancements in AI systems posed an increased risk of biological weapons attacks, relevant parts of the government would be promptly alerted, allowing for a rapid response to safeguard the public’s welfare.</p>

<h2 class="wp-block-heading has-text-align-none">Reporting mechanisms for real-world AI incidents</h2>

<p class="has-text-align-none">The US government currently lacks a comprehensive understanding of adverse incidents where AI systems have caused harm, hindering its ability to identify patterns of risky use, assess government guidelines, and respond to threats effectively. This blind spot leaves policymakers ill-equipped to craft timely and informed response measures.&nbsp;</p>

<p class="has-text-align-none">Establishing a voluntary national <a href="https://fas.org/publication/establishing-an-ai-incident-reporting-system/">AI incident reporting hub</a> would create a standardized channel for companies, researchers, and the public to confidentially report AI incidents, including system failures, accidents, misuse, and potential hazards. This hub would be housed at the National Institute of Standards and Technology, leveraging existing expertise in incident reporting and standards-setting while avoiding mandates; this will encourage collaborative industry participation.&nbsp;</p>

<p class="has-text-align-none">Combining this real-world data on adverse AI incidents with forward-looking capabilities reporting and researcher protections would enable the government to develop better informed policy responses to emerging AI issues and further empower developers to better understand the threats.</p>

<h2 class="wp-block-heading has-text-align-none">The path forward</h2>

<p class="has-text-align-none">These three proposals strike a balance between oversight and innovation in AI development. By incentivizing independent research and improving government visibility into AI capabilities and incidents, they could support both safety and technological advancement. The government could foster public trust and potentially accelerate AI adoption across sectors while preventing the regulatory backlash that could follow preventable high-profile incidents. Policymakers would be able to craft targeted regulations that address specific risks — such as AI-enhanced cyber threats or potential misuse in critical infrastructure — while preserving the flexibility needed for continued innovation in fields like health care diagnostics and climate modeling.</p>

<p class="has-text-align-none">Passing legislation in these areas requires bipartisan cooperation in Congress. Stakeholders from industry, academia, and civil society must advocate for and engage in this process, offering their expertise to refine and implement these proposals. There is a short <a href="https://www.nbcnews.com/politics/congress/chuck-schumer-eyes-opportunities-pass-deepfake-ai-bills-2024-elections-rcna164915">window for action</a> in what remains of the 118th Congress, with the potential to attach some AI transparency policies to must-pass legislation like the National Defense Authorization Act. The clock is ticking, and swift, decisive action now could set the stage for better AI governance for years to come.</p>

<p class="has-text-align-none">Imagine a future in which our government has the tools to understand and responsibly guide AI development and a future in which we can harness AI’s potential to solve grand challenges while safeguarding against risks. This future is within our grasp — but only if we act now to clear the fog and sharpen our collective vision of how AI is developed and used. By improving our collective understanding and oversight of AI, we increase our chances of steering this powerful technology toward beneficial outcomes for society.</p>
						
									]]></description><link>https://www.vox.com/future-perfect/368537/ai-artificial-intelligence-capabilities-risks-warning-system</link><guid isPermaLink="true">https://www.vox.com/future-perfect/368537/ai-artificial-intelligence-capabilities-risks-warning-system</guid><dc:creator><![CDATA[Jack Titus]]></dc:creator><pubDate>Mon, 26 Aug 2024 11:30:00 GMT</pubDate></item><item><title><![CDATA[We should have the ability to create crucial technologies on our own: MeitY Secretary]]></title><description><![CDATA[Hykon India Ltd., in collaboration with C-DAC (T), launched Hybrid Power Conditioning Systems for Microgrids. These systems, developed under the NaMPET programme, offer resilient industrial microgrid solutions with robust hardware, advanced cooling systems, and grid synchronisation features.]]></description><link>https://auto.economictimes.indiatimes.com/news/auto-technology/we-should-have-the-ability-to-create-crucial-technologies-on-our-own-meity-secretary/112806744</link><guid isPermaLink="true">https://auto.economictimes.indiatimes.com/news/auto-technology/we-should-have-the-ability-to-create-crucial-technologies-on-our-own-meity-secretary/112806744</guid><pubDate>Mon, 26 Aug 2024 11:06:41 GMT</pubDate></item><item><title><![CDATA[How to tell if your online accounts have been hacked]]></title><description><![CDATA[<p>This is a guide on how to check whether someone compromised your online accounts.</p>
<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>
]]></description><link>https://techcrunch.com/2024/08/26/how-to-tell-if-your-online-accounts-have-been-hacked/</link><guid isPermaLink="true">https://techcrunch.com/2024/08/26/how-to-tell-if-your-online-accounts-have-been-hacked/</guid><pubDate>Mon, 26 Aug 2024 11:30:09 GMT</pubDate></item><item><title><![CDATA[French police have arrested the founder of Telegram. What happens next could change the course of big tech - The Conversation France]]></title><description><![CDATA[<ol><li><a href="https://news.google.com/rss/articles/CBMi1wFBVV95cUxNc2kxLWx1T1pXWmw1OWJmb2NacVdYdjNYTFRqOUlFSzd6dDgyVXd3ejZDNFA4cXFrRlo0UDV4Q0U5N1RLaEZSYUFCZ1dIS0swbmZBWFctM3dyWXlBVVMzQXVYLXJ2R3A5NkxIcWJadGt1Vk5HTFBoZnU1ZUJJdXVXQkE2VE03M25VZjdFblZsSEY0azdyczZDQXFvZFk2X2Z4UEMtLWJNOHFlOVBJdVplU25wbmxxTVphUmZha1dmejJfekNkZkZQc0RUMElhR3pjOTVfMm9PVQ?oc=5" target="_blank">French police have arrested the founder of Telegram. What happens next could change the course of big tech</a>&nbsp;&nbsp;<font color="#6f6f6f">The Conversation France</font></li><li><a href="https://news.google.com/rss/articles/CBMikAFBVV95cUxNbURxYXdWdEtLTkZKLWhRWjl3Sl9Lc2tmcXlQYWFEaE15bG4waTdPeDkwSENKRkI0bVR3NFNoSk9JdjhKbnlJcUVTQzZ1YmNrcDdZbU9FUzdmb3YtdzN2UGFUdlBFWWY5VHBHRVVUMlcxRGZVdmpmcG9JYWE0YTlJUVlBalVQeWo3TnVIR3RjS3I?oc=5" target="_blank">Telegram Becomes Free Speech Flashpoint After Founder’s Arrest</a>&nbsp;&nbsp;<font color="#6f6f6f">The New York Times</font></li><li><a href="https://news.google.com/rss/articles/CBMilAFBVV95cUxNM3N4M0ZsVGF4QjllUDdpTjNBZUZHbkZ2YUVjb3ZkdEx4a3Y2VWYtbTYyNU90N3Jzc0lWZ3dNTkxwUENqclNoMWZsZVBtQWNvTUprcjEzNkIxVmw3MnVrbTdZM094TzY0UUZwTGRYUE8zVEdjNm11TF9ZNTExR3loRWlQeXZFQTk2b29oWWVYdWdFSVVP?oc=5" target="_blank">Who is Pavel Durov, the Russian-born Telegram founder detained in France?</a>&nbsp;&nbsp;<font color="#6f6f6f">The Washington Post</font></li><li><a href="https://news.google.com/rss/articles/CBMivwFBVV95cUxPTW44M3BXT29qemkxZlMzRWxTTlVaelZmVVFreFBPVE83RFlSRjVNV0NXb0F1WW5ORE1ldUh3dGNlQUo5SjFmaXVKUUg1XzNhLWtHaWNuT3F1aFZaNEdHLUZxMzVXamdGMG9vbDV2YWFVUVcyMUdGeXBPdUNzNW5menM5SHl5ODFpVW9scFZncGJVdnNUN3pLNVg3dEx4WDB3SlpBNXFjTDN0Nk9sRjVPc0hRcEduTWlHWjZOZVF0MNIBxAFBVV95cUxQYXBLZTdwaDg0WnViZGo5OHZkS3BUUFNZZ2dvamt1LVNPeXVmcXVJemQ0Ym1Vc2pvM1VDNWFhazAwelZFcGFwYzN0M0JBcF9HVnNBa0QzbTFIS0E4MFdTTmo2VUJhWHBOUHJWcmJNM3N1Qm02djRiZG1XRGhLc09IdXQ4aXVJeEJXc093eFBDb2dNY20yS1k0bGtEdWVUVzkwUlZKZE1ZS3pXeFUxTk1zZ2l2Q0lWWWlYTUxvclY0dS1HekF1?oc=5" target="_blank">Vindman says Musk should be 'nervous' after Telegram CEO was arrested: 'Free speech absolutists weirdos'</a>&nbsp;&nbsp;<font color="#6f6f6f">Fox News</font></li><li><a href="https://news.google.com/rss/articles/CBMinAFBVV95cUxOc0daSFJmcDJPc1k5MU5HNG1MU0lyYVU3S2lyd1lVV1UwQi1QM3QwZ3NPZlRWZUlramR6UDhBNzV1NFIwOTJ2S0w5MXkxYnFqeV9RNEF0M0laaWdoLTdWRjNfd2JadmtvQjUyUEd3SUZFLUJoazdLNkNLUmJIWHFfSzFHTG5UaHFkNWlLcW9ObmllcDdtY3U3Z2RSUlnSAaIBQVVfeXFMT1RhWTdiWVJJT2dNVHpVRjhIQ0JVVHhkQ0xFSzA4ZGo1bDNmbHZRc0k3T1NfRVo3ajhjbUJ4WkpJeE9EeDFDS3h1VXoyaFU3WThESE1UYU9lMlFkRE83di1iNmFLRVlsZlFncDdhMkt2V0FLRk4tRXhsOUl5aEtlYzlmX2laVFljN05TbmNtd0ZZLWlxU0NsU1pJSG52ZXVGdFVB?oc=5" target="_blank">Who is Telegram founder Pavel Durov — and why was he arrested?</a>&nbsp;&nbsp;<font color="#6f6f6f">CNBC</font></li></ol>]]></description><link>https://news.google.com/rss/articles/CBMi1wFBVV95cUxNc2kxLWx1T1pXWmw1OWJmb2NacVdYdjNYTFRqOUlFSzd6dDgyVXd3ejZDNFA4cXFrRlo0UDV4Q0U5N1RLaEZSYUFCZ1dIS0swbmZBWFctM3dyWXlBVVMzQXVYLXJ2R3A5NkxIcWJadGt1Vk5HTFBoZnU1ZUJJdXVXQkE2VE03M25VZjdFblZsSEY0azdyczZDQXFvZFk2X2Z4UEMtLWJNOHFlOVBJdVplU25wbmxxTVphUmZha1dmejJfekNkZkZQc0RUMElhR3pjOTVfMm9PVQ?oc=5</link><guid isPermaLink="true">https://news.google.com/rss/articles/CBMi1wFBVV95cUxNc2kxLWx1T1pXWmw1OWJmb2NacVdYdjNYTFRqOUlFSzd6dDgyVXd3ejZDNFA4cXFrRlo0UDV4Q0U5N1RLaEZSYUFCZ1dIS0swbmZBWFctM3dyWXlBVVMzQXVYLXJ2R3A5NkxIcWJadGt1Vk5HTFBoZnU1ZUJJdXVXQkE2VE03M25VZjdFblZsSEY0azdyczZDQXFvZFk2X2Z4UEMtLWJNOHFlOVBJdVplU25wbmxxTVphUmZha1dmejJfekNkZkZQc0RUMElhR3pjOTVfMm9PVQ?oc=5</guid><pubDate>Mon, 26 Aug 2024 05:59:54 GMT</pubDate></item><item><title><![CDATA[Supposed Pixel 9a images leak hinting at removed camera bar, boxy design - 9to5Google]]></title><description><![CDATA[<ol><li><a href="https://news.google.com/rss/articles/CBMiXEFVX3lxTE1wMnlOUi04TW9HNzhtSXNHbERwUTVaZmdWWGVEMlM4TUVtM043Mzh0enFMN3RldkM3Q2RJcEJyd2dMcmNHNjQyYTJVWlFhMEJnNXYzWGVsajNKa3h5?oc=5" target="_blank">Supposed Pixel 9a images leak hinting at removed camera bar, boxy design</a>&nbsp;&nbsp;<font color="#6f6f6f">9to5Google</font></li><li><a href="https://news.google.com/rss/articles/CBMipgFBVV95cUxOcGJSbFdVWmIxcHJIWmNCU0FRSl9SNV8teVY5WlVXTHA1dDBZdFpyNE1sZ1JZTi0xS2E3ellnQVFQYjE2c3NkeE9jRW9ZS3p0Q2JjX1piemdCbGQzZ1FTVURmRzMxSEc3UEtyMFlzbnp1TjhUTVBVb3ExbUdzSGtoZlFNdVR6VHRTYWtuQzktQ09nbW1FLXdaaHBUeFBmRE90NDVRYWhR?oc=5" target="_blank">Believe it or not, Google's Pixel 9a may have already leaked out in the wild with redesigned camera</a>&nbsp;&nbsp;<font color="#6f6f6f">PhoneArena</font></li><li><a href="https://news.google.com/rss/articles/CBMikwFBVV95cUxNbmlGUFNoelhhX1pCckhaa1FQbE9RbmU0MUVNNXBYR3BlWk1TX0QydmhKODRMUmxpOTBnWi1scDUyNjRBaGZUczFhbGNLX2hvME91aEU5TkhiWm1JX2VnNS1nazZ4emNkc2dFdkQ1Q1k3Y2ZrME40WkFzZWNoT25SZTljM180ZlQ4ODdybXFMelBXNU3SAZgBQVVfeXFMUElPSzRrWUxGekhIMzJJblZlbjRCUzZmamxoRDZza2JjMTJLQVhmUDdwV2tMX1RxUS1fbmV0WVhXUm5tWjBNLWVBdkNwRHpmQVN4RU9GbWNHVWVkNU8wMktESlZfQUxacW5zajVCWXBKUi1tTUROWm4yek56SEVWRmpFR2xiSmlUZGd4NkJCZTNYRDBUYXAyTGE?oc=5" target="_blank">Google Pixel 9a has appeared in photos for the first time</a>&nbsp;&nbsp;<font color="#6f6f6f">gagadget.com</font></li><li><a href="https://news.google.com/rss/articles/CBMic0FVX3lxTFB5Y1JDZU51MWJJUENzeVZlSkZxQzZnTm51b29PT1ZxSFhDZFpKTkRsTnQ0WGpVdmRLX0dOVjJmWk5JMDlBX3ZRR2xHeTI3S3U0RGZ6ZGFZVVZNQVUzTGs3ajFOb1hJQV9RRGp5S3hRVzI2WEk?oc=5" target="_blank">Here's your first look at the Google Pixel 9a</a>&nbsp;&nbsp;<font color="#6f6f6f">Android Police</font></li></ol>]]></description><link>https://news.google.com/rss/articles/CBMiXEFVX3lxTE1wMnlOUi04TW9HNzhtSXNHbERwUTVaZmdWWGVEMlM4TUVtM043Mzh0enFMN3RldkM3Q2RJcEJyd2dMcmNHNjQyYTJVWlFhMEJnNXYzWGVsajNKa3h5?oc=5</link><guid isPermaLink="true">https://news.google.com/rss/articles/CBMiXEFVX3lxTE1wMnlOUi04TW9HNzhtSXNHbERwUTVaZmdWWGVEMlM4TUVtM043Mzh0enFMN3RldkM3Q2RJcEJyd2dMcmNHNjQyYTJVWlFhMEJnNXYzWGVsajNKa3h5?oc=5</guid><pubDate>Mon, 26 Aug 2024 09:21:00 GMT</pubDate></item><item><title><![CDATA[Intel ‘Lunar Lake’ Core Ultra 200V laptops from Acer and Asus spotted in retail listings]]></title><link>https://www.yahoo.com/tech/intel-lunar-lake-core-ultra-142800846.html</link><guid isPermaLink="true">https://www.yahoo.com/tech/intel-lunar-lake-core-ultra-142800846.html</guid><pubDate>Sun, 25 Aug 2024 14:28:00 GMT</pubDate></item><item><title><![CDATA[Why Lockheed Martin and General Dynamics Just Declared War on Rocket Engines]]></title><link>https://finance.yahoo.com/news/why-lockheed-martin-general-dynamics-160600287.html</link><guid isPermaLink="true">https://finance.yahoo.com/news/why-lockheed-martin-general-dynamics-160600287.html</guid><pubDate>Sun, 25 Aug 2024 16:06:00 GMT</pubDate></item><item><title><![CDATA[How Big Tech is approaching explicit, nonconsensual deepfakes]]></title><description><![CDATA[With the White House demanding more action, tech and social media companies are slowly figuring out how to address the spread of nonconsensual explicit deepfakes — and their role in the system.]]></description><link>https://mashable.com/article/big-tech-social-media-explicit-deepfake-policies</link><guid isPermaLink="true">https://mashable.com/article/big-tech-social-media-explicit-deepfake-policies</guid><pubDate>Mon, 26 Aug 2024 11:00:00 GMT</pubDate></item></channel></rss>