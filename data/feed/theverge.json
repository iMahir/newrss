[
  {
    "title": "Google’s AI enters its ‘agentic era’",
    "link": "https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent",
    "pubDate": "2024-12-11T15:33:02.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"A woman holding a phone, and to the right, her phone is overlayed and displayed. On the phone, she’s recording her surroundings with Google’s AI product, Project Astra.\" src=\"https://cdn.vox-cdn.com/thumbor/HGvl11UuqpE8qjquvo5ZJP25Hx4=/363x6:1920x1044/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780393/TT_ASH_STILL_FINAL.0.jpeg\" />\n        <figcaption>Image: Google</figcaption>\n    </figure>\n\n  <p id=\"yh0QVL\">I stepped into a room lined with bookshelves, stacked with ordinary programming and architecture texts. One shelf stood slightly askew, and behind it was a hidden room that had three TVs displaying famous artworks: Edvard Munch’s <a href=\"https://www.edvardmunch.org/the-scream.jsp\"><em>The Scream</em></a>, Georges Seurat’s <a href=\"https://www.artic.edu/artworks/27992/a-sunday-on-la-grande-jatte-1884\"><em>Sunday Afternoon</em></a>, and Hokusai’s <a href=\"https://en.wikipedia.org/wiki/The_Great_Wave_off_Kanagawa\"><em>The Great Wave off Kanagawa</em></a>. “There’s some interesting pieces of art here,” said Bibo Xu, Google DeepMind’s lead product manager for Project Astra. “Is there one in particular that you would want to talk about?” </p>\n<p id=\"JtPKZx\">Project Astra, Google’s prototype AI “universal agent,” responded smoothly. “The <em>Sunday Afternoon </em>artwork was discussed previously,” it replied. “Was there a particular detail about it you wish to discuss, or were you interested in discussing <em>The Scream</em>?”</p>\n<p id=\"mkgXk5\">I was at Google’s sprawling Mountain View campus, seeing the latest projects from its AI lab DeepMind. One was Project Astra, a virtual assistant <a href=\"https://www.theverge.com/2024/5/14/24156296/google-ai-gemini-astra-assistant-live-io\">first demoed at Google I/O</a> earlier this year. Currently contained in an app, it can process text, images, video, and audio in real time and respond to questions about them. It’s like a Siri or Alexa that’s slightly more natural to talk to, can see the world around you, and can “remember”...</p>\n  <p><a href=\"https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent\">Read the full story at The Verge.</a></p>\n\n",
    "summary": "Image: Google\n    \n\n  \nI stepped into a room lined with bookshelves, stacked with ordinary programming and architecture texts. One shelf stood slightly askew, and behind it was a hidden room that had three TVs displaying famous artworks: Edvard Munch’s The Scream, Georges Seurat’s Sunday Afternoon, and Hokusai’s The Great Wave off Kanagawa. “There’s some interesting pieces of art here,” said Bibo Xu, Google DeepMind’s lead product manager for Project Astra. “Is there one in particular that you would want to talk about?” \nProject Astra, Google’s prototype AI “universal agent,” responded smoothly. “The Sunday Afternoon artwork was discussed previously,” it replied. “Was there a particular detail about it you wish to discuss, or were you interested in discussing The Scream?”\nI was at Google’s sprawling Mountain View campus, seeing the latest projects from its AI lab DeepMind. One was Project Astra, a virtual assistant first demoed at Google I/O earlier this year. Currently contained in an app, it can process text, images, video, and audio in real time and respond to questions about them. It’s like a Siri or Alexa that’s slightly more natural to talk to, can see the world around you, and can “remember”...\nRead the full story at The Verge.",
    "author": "Kylie Robison"
  },
  {
    "title": "It sure sounds like Google is planning to actually launch some smart glasses",
    "link": "https://www.theverge.com/2024/12/11/24318672/google-smart-glasses-ai-gemini",
    "pubDate": "2024-12-11T15:30:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"A man in a bike helmet, wearing glasses.\" src=\"https://cdn.vox-cdn.com/thumbor/QZUN81v654TmFeyekOgmYrAdLMc=/70x0:1690x1080/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780383/Google_Glasses.0.png\" />\n        <figcaption><em>Here’s what Google’s latest smart glasses prototype looks like.</em> | Image: Google</figcaption>\n    </figure>\n\n  <p id=\"9peXhl\">Google is working on a lot of AI stuff — like, a <em>lot </em>of AI stuff — but if you want to really understand the company’s vision for virtual assistants, take a look at Project Astra. Google first showed a demo of its all-encompassing, multimodal virtual assistant at Google I/O this spring and clearly imagines Astra as an always-on helper in your life. In reality, the tech is somewhere between “neat concept video” and “early prototype,” but it represents the most ambitious version of Google’s AI work. </p>\n<p id=\"gFgV3p\">And there’s one thing that keeps popping up in Astra demos: glasses. Google has been working on smart facewear of one kind or another for years, from Glass to Cardboard to the <a href=\"https://www.theverge.com/2022/7/19/23270219/google-ar-prototypes-test-public\">Project Iris</a> translator glasses it showed off two years ago. Earlier this year, all Google spokesperson Jane Park would tell us was that the glasses were “<a href=\"https://www.theverge.com/2024/5/14/24156518/google-glass-prototype-ar-glasses-io-2024\">a functional research prototype</a>.” </p>\n<p id=\"nbPDvv\">Now, they appear to be something at least a little more than that. During a press briefing ahead of the launch of Gemini 2.0, Bibo Xu, a product manager on the Google DeepMind team, said that “a small group will be testing Project Astra on prototype glasses, which we believe is one of the most powerful and intuitive form factors to experience this kind of AI.” That group will be part of Google’s Trusted Tester program, which often gets access to these early prototypes, many of which don’t ever ship publicly. Some testers will use Astra on an Android phone; others through the glasses.</p>\n<div id=\"TGH9Ac\"><div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\"><iframe src=\"https://www.youtube.com/embed/hIIlJt8JERI?rel=0\" style=\"top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;\" allowfullscreen=\"\" scrolling=\"no\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;\"></iframe></div></div>\n<p id=\"TmdjhB\">Later in the briefing, in response to a question about the glasses, Xu said that “for the glasses product itself, we’ll have more news coming shortly.” Is that definitive proof that Google Smart Glasses are coming to a store near you sometime soon? Of course not! But it certainly indicates that Google has some hardware plans for Project Astra.</p>\n<p id=\"N9VhLr\">Smart glasses make perfect sense for what Google is trying to do with Astra. There’s simply no better way to combine audio, video, and a display than on a device on your face — especially if you’re hoping for something like an always-on experience. In a new video showing Astra’s capabilities with Gemini 2.0, a tester uses Astra to remember security codes at an apartment building, check the weather, and much more. At one point, he sees a bus flying past and asks Astra if “that bus will take me anywhere near Chinatown.” It’s all the sort of thing you <em>can </em>do with a phone, but nearly all of it feels far more natural through a wearable.</p>\n<p id=\"14be84\">Right now, smart glasses like these — and like <a href=\"https://www.theverge.com/24253908/meta-orion-ar-glasses-demo-mark-zuckerberg-interview\">Meta’s Orion</a> — are mostly vaporware. When they’ll ship, <em>whether </em>they’ll ship, and whether they’ll be any good all remains up in the air. But Google is dead serious about making smart glasses work. And seems to be just as serious about making the smart glasses itself.</p>\n\n",
    "summary": "Here’s what Google’s latest smart glasses prototype looks like. | Image: Google\n    \n\n  \nGoogle is working on a lot of AI stuff — like, a lot of AI stuff — but if you want to really understand the company’s vision for virtual assistants, take a look at Project Astra. Google first showed a demo of its all-encompassing, multimodal virtual assistant at Google I/O this spring and clearly imagines Astra as an always-on helper in your life. In reality, the tech is somewhere between “neat concept video” and “early prototype,” but it represents the most ambitious version of Google’s AI work. \nAnd there’s one thing that keeps popping up in Astra demos: glasses. Google has been working on smart facewear of one kind or another for years, from Glass to Cardboard to the Project Iris translator glasses it showed off two years ago. Earlier this year, all Google spokesperson Jane Park would tell us was that the glasses were “a functional research prototype.” \nNow, they appear to be something at least a little more than that. During a press briefing ahead of the launch of Gemini 2.0, Bibo Xu, a product manager on the Google DeepMind team, said that “a small group will be testing Project Astra on prototype glasses, which we believe is one of the most powerful and intuitive form factors to experience this kind of AI.” That group will be part of Google’s Trusted Tester program, which often gets access to these early prototypes, many of which don’t ever ship publicly. Some testers will use Astra on an Android phone; others through the glasses.\n\n\nLater in the briefing, in response to a question about the glasses, Xu said that “for the glasses product itself, we’ll have more news coming shortly.” Is that definitive proof that Google Smart Glasses are coming to a store near you sometime soon? Of course not! But it certainly indicates that Google has some hardware plans for Project Astra.\nSmart glasses make perfect sense for what Google is trying to do with Astra. There’s simply no better way to combine audio, video, and a display than on a device on your face — especially if you’re hoping for something like an always-on experience. In a new video showing Astra’s capabilities with Gemini 2.0, a tester uses Astra to remember security codes at an apartment building, check the weather, and much more. At one point, he sees a bus flying past and asks Astra if “that bus will take me anywhere near Chinatown.” It’s all the sort of thing you can do with a phone, but nearly all of it feels far more natural through a wearable.\nRight now, smart glasses like these — and like Meta’s Orion — are mostly vaporware. When they’ll ship, whether they’ll ship, and whether they’ll be any good all remains up in the air. But Google is dead serious about making smart glasses work. And seems to be just as serious about making the smart glasses itself.",
    "author": "David Pierce"
  },
  {
    "title": "Google’s new Jules AI agent will help developers fix buggy code",
    "link": "https://www.theverge.com/2024/12/11/24318628/jules-google-ai-coding-agent-gemini-2-0-announcement",
    "pubDate": "2024-12-11T15:30:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"An illustration of Google’s multicolor “G” logo\" src=\"https://cdn.vox-cdn.com/thumbor/wEhfAvC2MonJj2pXsrH-puCX3SU=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780369/STK093_Google_01.0.jpg\" />\n        <figcaption>Illustration: The Verge</figcaption>\n    </figure>\n\n  <p id=\"oGsece\">Google has announced an experimental AI-powered code agent called “Jules” that can automatically fix coding errors for developers. <a href=\"https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers\">Jules was introduced today</a> alongside Gemini 2.0, and uses the updated Google AI model to create multi-step plans to address issues, modify multiple files, and prepare pull requests for Python and Javascript coding tasks in GitHub workflows.</p>\n<p id=\"BI4rXb\">Microsoft introduced <a href=\"https://www.theverge.com/2023/3/22/23651456/github-copilot-x-gpt-4-code-chat-voice-support\">a similar experience for GitHub Copilot last year</a> that can recognize and explain code, alongside recommending changes and fixing bugs. Jules will compete against Microsoft’s offering, and also against tools like <a href=\"https://www.cursor.com/\">Cursor</a> and even Claude and ChatGPT’s coding abilities. Google’s launch of a coding-focused AI assistant is no surprise — <a href=\"https://www.theverge.com/2024/10/29/24282757/google-new-code-generated-ai-q3-2024\">CEO Sundar Pichai said in October</a> that more than a quarter of all new code at the company is now generated by AI.</p>\n<p id=\"OPzYlw\">“Jules handles bug fixes and other time-consuming tasks while you focus on what you actually want to build,” Google says in its blog post. “This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.”</p>\n<p id=\"SsLVQy\">Developers have full control to review and adjust the plans created by Jules, before choosing to merge the code it generates into their projects. The announcement doesn’t say that Jules will <em>spot </em>bugs for you, so presumably it needs to be directed to a list of issues that have already been identified to fix. Google also says that Jules is in early development and “may make mistakes,” but internal testing has shown it’s been beneficial for boosting developer productivity and providing real-time updates to help track and manage tasks.</p>\n<p id=\"e4WGaG\">Jules is launching today for a “select group of trusted testers” according to Google, and will be released to other developers in early 2025. Updates about availability and how development is progressing will be available via the <a href=\"http://labs.google.com/jules\">Google Labs website</a>.</p>\n\n",
    "summary": "Illustration: The Verge\n    \n\n  \nGoogle has announced an experimental AI-powered code agent called “Jules” that can automatically fix coding errors for developers. Jules was introduced today alongside Gemini 2.0, and uses the updated Google AI model to create multi-step plans to address issues, modify multiple files, and prepare pull requests for Python and Javascript coding tasks in GitHub workflows.\nMicrosoft introduced a similar experience for GitHub Copilot last year that can recognize and explain code, alongside recommending changes and fixing bugs. Jules will compete against Microsoft’s offering, and also against tools like Cursor and even Claude and ChatGPT’s coding abilities. Google’s launch of a coding-focused AI assistant is no surprise — CEO Sundar Pichai said in October that more than a quarter of all new code at the company is now generated by AI.\n“Jules handles bug fixes and other time-consuming tasks while you focus on what you actually want to build,” Google says in its blog post. “This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.”\nDevelopers have full control to review and adjust the plans created by Jules, before choosing to merge the code it generates into their projects. The announcement doesn’t say that Jules will spot bugs for you, so presumably it needs to be directed to a list of issues that have already been identified to fix. Google also says that Jules is in early development and “may make mistakes,” but internal testing has shown it’s been beneficial for boosting developer productivity and providing real-time updates to help track and manage tasks.\nJules is launching today for a “select group of trusted testers” according to Google, and will be released to other developers in early 2025. Updates about availability and how development is progressing will be available via the Google Labs website.",
    "author": "Jess Weatherbed"
  },
  {
    "title": "Google is testing Gemini AI agents that help you in video games",
    "link": "https://www.theverge.com/2024/12/11/24318530/google-gemini-2-0-understand-rules-video-games-genie",
    "pubDate": "2024-12-11T15:30:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"Promotional art for Clash of Clans.\" src=\"https://cdn.vox-cdn.com/thumbor/cX3NSYMJRchmIymGKsOKgx9ubtQ=/82x0:969x591/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780368/fc_16x9_Super_Wall_Breakers_Even.0.png\" />\n        <figcaption><em>Clash of Clans.</em> | Image: Supercell</figcaption>\n    </figure>\n\n  <p id=\"29VbeG\">Google just announced Gemini 2.0, and as part of its suite of news today, the company is revealing that it’s been exploring how AI agents built with Gemini 2.0 can understand rules in video games to help you out.</p>\n<p id=\"LhYnpp\">The agents can “reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation,” Google DeepMind CEO Demis Hassabis and CTO Koray Kavukcuoglu write <a href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024\">in a blog post</a>. Hassabis and Kavukcuoglu also say that the agents can also “tap into Google Search to connect you with the wealth of gaming knowledge on the web.”</p>\n<p id=\"RYJRFc\">Google is testing the agents’ “ability to interpret rules and challenges” in games like <em>Clash of Clans</em> and <em>Hay Day </em>from Supercell<em>, </em>according to Hassabis and Kavukcuoglu.</p>\n<p id=\"Bdb9uA\">I’m not surprised Google is chasing these ideas: in theory, an AI agent coaching you through a strategy or puzzle could be useful. It sounds like this work is very early, though, and I have many questions about whether or not these agents actually give sound advice.</p>\n<p id=\"2Obiyz\">Google is also investing in video games and AI in another way: creating playable virtual worlds on the fly from a prompt image using a “foundation world model” called Genie 2 <a href=\"https://www.theverge.com/2024/12/4/24313409/a-google-ai-model-can-create-playable-virtual-worlds-on-the-fly\">that it showed off last week</a>. That work seems early, too: Genie 2 can only generate consistent worlds for “up to a minute,” <a href=\"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\">Google says</a>.</p>\n\n",
    "summary": "Clash of Clans. | Image: Supercell\n    \n\n  \nGoogle just announced Gemini 2.0, and as part of its suite of news today, the company is revealing that it’s been exploring how AI agents built with Gemini 2.0 can understand rules in video games to help you out.\nThe agents can “reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation,” Google DeepMind CEO Demis Hassabis and CTO Koray Kavukcuoglu write in a blog post. Hassabis and Kavukcuoglu also say that the agents can also “tap into Google Search to connect you with the wealth of gaming knowledge on the web.”\nGoogle is testing the agents’ “ability to interpret rules and challenges” in games like Clash of Clans and Hay Day from Supercell, according to Hassabis and Kavukcuoglu.\nI’m not surprised Google is chasing these ideas: in theory, an AI agent coaching you through a strategy or puzzle could be useful. It sounds like this work is very early, though, and I have many questions about whether or not these agents actually give sound advice.\nGoogle is also investing in video games and AI in another way: creating playable virtual worlds on the fly from a prompt image using a “foundation world model” called Genie 2 that it showed off last week. That work seems early, too: Genie 2 can only generate consistent worlds for “up to a minute,” Google says.",
    "author": "Jay Peters"
  },
  {
    "title": "Google launched Gemini 2.0, its new AI model for practically everything",
    "link": "https://www.theverge.com/2024/12/11/24318444/google-gemini-2-0-flash-ai-model",
    "pubDate": "2024-12-11T15:30:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"Vector illustration of the Google Gemini logo.\" src=\"https://cdn.vox-cdn.com/thumbor/a-iqXhjLSu3ID3fJsw-0PpvgCuc=/20x0:2021x1334/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780371/STK255_Google_Gemini_C.0.jpg\" />\n        <figcaption>Illustration: The Verge</figcaption>\n    </figure>\n\n  <p id=\"21tGpz\">Google’s latest AI model has a lot of work to do. Like <a href=\"https://www.theverge.com/23610427/chatbots-chatgpt-new-bing-google-bard-conversational-ai\">every other company</a> in the AI race, Google is frantically building AI into practically every product it owns, trying to build products other developers want to use, and racing to set up all the infrastructure to make those things possible without being so expensive it runs the company out of business. Meanwhile, Amazon, Microsoft, Anthropic, and OpenAI are <a href=\"https://www.theverge.com/24278413/ai-manifesto-anthropic-dario-amodei-agi-digital-god-openai-sam-altman-decoder-podcast\">pouring their own billions</a> into pretty much the exact same set of problems. </p>\n<p id=\"v2jVay\">That may explain why Demis Hassabis, the CEO of Google DeepMind and the head of all the company’s AI efforts, is so excited about how all-encompassing the new Gemini 2.0 model is. Google <a href=\"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024\">is releasing Gemini 2.0</a> on Wednesday, about 10 months after the company <a href=\"https://www.theverge.com/2024/2/15/24073457/google-gemini-1-5-ai-model-llm\">first launched 1.5</a>. It’s still in what Google calls an “experimental preview,” and only one version of the model — the smaller, lower-end 2.0 Flash — is being released. But Hassabis says it’s still a big day.</p>\n<p id=\"uiE4fp\">“Effectively,” Hassabis says, “it’s as good as the current Pro model is. So you can think of it as one whole tier better, for the same cost efficiency and performance efficiency and speed. We’re really happy with that.” And not only is it better at doing the old things Gemini could do but it can also do new things. Gemini 2.0 can now natively generate audio and images, and it brings new multimodal capabilities that Hassabis says lay the groundwork for the next big thing in AI: agents.</p>\n<p id=\"ZOObHN\">Agentic AI, as everyone calls it, refers to AI bots that can actually go off and accomplish things on your behalf. Google has been demoing one, Project Astra, <a href=\"https://www.theverge.com/2024/5/14/24156296/google-ai-gemini-astra-assistant-live-io\">since this spring</a> — it’s a visual system that can identify objects, help you navigate the world, and tell you where you left your glasses. Gemini 2.0 represents a huge improvement for Astra, Hassabis says. </p>\n<p id=\"R9r4Fo\">Google is <a href=\"https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent\">also launching Project Mariner</a>, an experimental new Chrome extension that can quite literally use your web browser for you. There’s also <a href=\"https://www.theverge.com/2024/12/11/24318628/jules-google-ai-coding-agent-gemini-2-0-announcement\">Jules</a>, an agent specifically for helping developers find and fix bad code, and a new Gemini 2.0-based agent that can look at your screen and help you <a href=\"https://www.theverge.com/2024/12/11/24318530/google-gemini-2-0-understand-rules-video-games-genie\">better play video games</a>. Hassabis calls the game agent “an Easter egg” but also points to it as the sort of thing a truly multimodal, built-in model can do for you.</p>\n<p id=\"GGqWR1\">“We really see 2025 as the true start of the agent-based era,” Hassabis says, “and Gemini 2.0 is the foundation of that.” He’s careful to note that the performance isn’t the only upgrade here; as talk of an industrywide slowdown in model improvements continues, he says Google is still seeing gains as it trains new models, but he’s just as excited about the efficiency and speed improvements. </p>\n<div class=\"c-float-left c-float-hang\"><aside id=\"PM1JyZ\"><q>Google’s plan for Gemini 2.0 is to use it absolutely everywhere</q></aside></div>\n<p id=\"1GqEN4\">This won’t shock you, but Google’s plan for Gemini 2.0 is to use it absolutely everywhere. It will power AI Overviews in Google Search, which Google says now reach 1 billion people and which the company says will now be more nuanced and complex thanks to Gemini 2.0. It’ll be in the Gemini bot and app, of course, and will eventually power the AI features in Workspace and elsewhere at Google. Google has worked to bring as many features as possible into the model itself, rather than run a bunch of individual and siloed products, in order to be able to do more with Gemini in more places. The multimodality, the different kinds of outputs, the features — the goal is to get all of it into the foundational Gemini model. “We’re trying to build the most general model possible,” Hassabis says. </p>\n<p id=\"LUeaBs\">As the agentic era of AI begins, Hassabis says there are both new and old problems to solve. The old ones are eternal, about performance and efficiency and inference cost. The new ones are in many ways unknown. Just to name one: what safety risks will these agents pose out in the world operating of their own accord? Google is taking some precautions with Mariner and Astra, but Hassabis says there’s more research to be done. “We’re going to need new safety solutions,” he says, “like testing in hardened sandboxes. I think that’s going to be quite important for testing agents, rather than out in the wild… they’ll be more useful, but there will also be more risks.”</p>\n<p id=\"nlPQPN\">Gemini 2.0 may be in an experimental stage for now, but you can already use it by choosing the new model in the Gemini web app. (No word yet on when you’ll get to try the non-Flash models.) And early next year, Hassabis says, it’s coming for other Gemini platforms, everything else Google makes, and the whole internet.</p>\n<p id=\"oU44Sk\"></p>\n<p id=\"pwLFQ3\"></p>\n\n",
    "summary": "Illustration: The Verge\n    \n\n  \nGoogle’s latest AI model has a lot of work to do. Like every other company in the AI race, Google is frantically building AI into practically every product it owns, trying to build products other developers want to use, and racing to set up all the infrastructure to make those things possible without being so expensive it runs the company out of business. Meanwhile, Amazon, Microsoft, Anthropic, and OpenAI are pouring their own billions into pretty much the exact same set of problems. \nThat may explain why Demis Hassabis, the CEO of Google DeepMind and the head of all the company’s AI efforts, is so excited about how all-encompassing the new Gemini 2.0 model is. Google is releasing Gemini 2.0 on Wednesday, about 10 months after the company first launched 1.5. It’s still in what Google calls an “experimental preview,” and only one version of the model — the smaller, lower-end 2.0 Flash — is being released. But Hassabis says it’s still a big day.\n“Effectively,” Hassabis says, “it’s as good as the current Pro model is. So you can think of it as one whole tier better, for the same cost efficiency and performance efficiency and speed. We’re really happy with that.” And not only is it better at doing the old things Gemini could do but it can also do new things. Gemini 2.0 can now natively generate audio and images, and it brings new multimodal capabilities that Hassabis says lay the groundwork for the next big thing in AI: agents.\nAgentic AI, as everyone calls it, refers to AI bots that can actually go off and accomplish things on your behalf. Google has been demoing one, Project Astra, since this spring — it’s a visual system that can identify objects, help you navigate the world, and tell you where you left your glasses. Gemini 2.0 represents a huge improvement for Astra, Hassabis says. \nGoogle is also launching Project Mariner, an experimental new Chrome extension that can quite literally use your web browser for you. There’s also Jules, an agent specifically for helping developers find and fix bad code, and a new Gemini 2.0-based agent that can look at your screen and help you better play video games. Hassabis calls the game agent “an Easter egg” but also points to it as the sort of thing a truly multimodal, built-in model can do for you.\n“We really see 2025 as the true start of the agent-based era,” Hassabis says, “and Gemini 2.0 is the foundation of that.” He’s careful to note that the performance isn’t the only upgrade here; as talk of an industrywide slowdown in model improvements continues, he says Google is still seeing gains as it trains new models, but he’s just as excited about the efficiency and speed improvements. \nGoogle’s plan for Gemini 2.0 is to use it absolutely everywhere\nThis won’t shock you, but Google’s plan for Gemini 2.0 is to use it absolutely everywhere. It will power AI Overviews in Google Search, which Google says now reach 1 billion people and which the company says will now be more nuanced and complex thanks to Gemini 2.0. It’ll be in the Gemini bot and app, of course, and will eventually power the AI features in Workspace and elsewhere at Google. Google has worked to bring as many features as possible into the model itself, rather than run a bunch of individual and siloed products, in order to be able to do more with Gemini in more places. The multimodality, the different kinds of outputs, the features — the goal is to get all of it into the foundational Gemini model. “We’re trying to build the most general model possible,” Hassabis says. \nAs the agentic era of AI begins, Hassabis says there are both new and old problems to solve. The old ones are eternal, about performance and efficiency and inference cost. The new ones are in many ways unknown. Just to name one: what safety risks will these agents pose out in the world operating of their own accord? Google is taking some precautions with Mariner and Astra, but Hassabis says there’s more research to be done. “We’re going to need new safety solutions,” he says, “like testing in hardened sandboxes. I think that’s going to be quite important for testing agents, rather than out in the wild… they’ll be more useful, but there will also be more risks.”\nGemini 2.0 may be in an experimental stage for now, but you can already use it by choosing the new model in the Gemini web app. (No word yet on when you’ll get to try the non-Flash models.) And early next year, Hassabis says, it’s coming for other Gemini platforms, everything else Google makes, and the whole internet.",
    "author": "David Pierce"
  },
  {
    "title": "Google built an AI tool that can do research for you",
    "link": "https://www.theverge.com/2024/12/11/24318217/google-gemini-advanced-deep-research-launch",
    "pubDate": "2024-12-11T15:30:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"Image of the Google “G” logo on a blue, black, and purple background.\" src=\"https://cdn.vox-cdn.com/thumbor/g5HvsCeZn9sMSwV_9yDzL_PqR_M=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780370/STK093_Google_03.0.jpg\" />\n        <figcaption>Illustration: The Verge</figcaption>\n    </figure>\n\n  <p id=\"pKZYbR\">Google has just revealed a new AI tool <a href=\"http://hps://blog.google/products/gemini/google-gemini-deep-research/\">called Deep Research</a> that lets you call upon its Gemini bot to scour the web for you and write a detailed report based on its findings.</p>\n<p id=\"OOYdiW\">Deep Research is currently only available in English to Gemini Advanced subscribers. If you have access, you can ask Gemini to research a particular topic on your behalf, and the chatbot will create a “multi-step research plan” that you can either edit or approve. Google says Gemini will start its research by “finding interesting pieces of information” on the web and then performing related searches — a process it repeats several times.</p>\n<figure class=\"e-image\">\n        <img alt=\" \" data-mask-text=\"false\" src=\"https://cdn.vox-cdn.com/thumbor/FXcThBc2tlONIRUrL2jsd_6xEJo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25784204/deep_research.gif\">\n      <cite>GIF: Google</cite>\n  </figure>\n<div class=\"c-float-left c-float-hang\"><aside id=\"UjZrEc\"><div data-anthem-component=\"readmore\" data-anthem-component-data='{\"stories\":[{\"title\":\"Agents are the future AI companies promise — and desperately need\",\"url\":\"https://www.theverge.com/2024/10/10/24266333/ai-agents-assistants-openai-google-deepmind-bots\"}]}'></div></aside></div>\n<p id=\"oFi60y\">When it’s finished, Gemini will spit out a report of its “key findings” with links to the websites where it found its information. You can ask Gemini to expand on certain areas or tweak its report, as well as export the AI-generated research to Google Docs. This all sounds a bit similar to the <a href=\"https://www.theverge.com/2024/5/30/24167986/perplexity-ai-research-pages-school-report\">Pages feature offered by the AI search engine Perplexity</a>, which generates a custom webpage based on your prompt.</p>\n<p id=\"WkVYdb\">Google took the wraps off Deep Research as part of a broader announcement for Gemini 2.0, its new model for an era of “agentic” AI, or the AI systems that can perform tasks for you. Deep Research is just one example of Google’s agentic push, and it’s <a href=\"https://www.theverge.com/2024/10/10/24266333/ai-agents-assistants-openai-google-deepmind-bots\">something other AI companies</a> are seriously exploring as well.</p>\n<p id=\"N43w9r\">Along with Deep Research, Google announced that it’s making Gemini Flash 2.0 — <a href=\"https://www.theverge.com/2024/7/25/24206071/google-gemini-ai-flash-upgrade\">a speedier version of the next-gen chatbot</a> — available to developers. Deep Research is currently only available for Gemini Advanced subscribers on the web. You can try it by heading to Gemini and then changing the model dropdown to “Gemini 1.5 Pro with Deep Research.” </p>\n\n",
    "summary": "Illustration: The Verge\n    \n\n  \nGoogle has just revealed a new AI tool called Deep Research that lets you call upon its Gemini bot to scour the web for you and write a detailed report based on its findings.\nDeep Research is currently only available in English to Gemini Advanced subscribers. If you have access, you can ask Gemini to research a particular topic on your behalf, and the chatbot will create a “multi-step research plan” that you can either edit or approve. Google says Gemini will start its research by “finding interesting pieces of information” on the web and then performing related searches — a process it repeats several times.\nGIF: Google\n  \n\n\nWhen it’s finished, Gemini will spit out a report of its “key findings” with links to the websites where it found its information. You can ask Gemini to expand on certain areas or tweak its report, as well as export the AI-generated research to Google Docs. This all sounds a bit similar to the Pages feature offered by the AI search engine Perplexity, which generates a custom webpage based on your prompt.\nGoogle took the wraps off Deep Research as part of a broader announcement for Gemini 2.0, its new model for an era of “agentic” AI, or the AI systems that can perform tasks for you. Deep Research is just one example of Google’s agentic push, and it’s something other AI companies are seriously exploring as well.\nAlong with Deep Research, Google announced that it’s making Gemini Flash 2.0 — a speedier version of the next-gen chatbot — available to developers. Deep Research is currently only available for Gemini Advanced subscribers on the web. You can try it by heading to Gemini and then changing the model dropdown to “Gemini 1.5 Pro with Deep Research.”",
    "author": "Emma Roth"
  },
  {
    "title": "Wallace & Gromit studio Aardman is working on a Pokémon project",
    "link": "https://www.theverge.com/2024/12/11/24318706/pokemon-aardman-project",
    "pubDate": "2024-12-11T15:11:11.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"A pokéball made of clay. Above the pokéball is the Pokémon logo, and below is is the logo for studio Aardman.\" src=\"https://cdn.vox-cdn.com/thumbor/Cc8rJrN4C3MMTIEnfePbORGRFgM=/227x0:2645x1612/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780325/Screenshot_2024_12_11_at_9.18.49_AM.0.png\" />\n        <figcaption>TCPi / Aardman</figcaption>\n    </figure>\n\n  <p id=\"3hZ1as\"><a href=\"https://www.theverge.com/2024/10/7/24264524/lets-stop-motion-and-take-another-look-over-the-garden-wall\">Aardman Animation</a>, the studio behind the <a href=\"https://www.theverge.com/2024/10/16/24271694/feathers-mcgraw-is-back-with-a-vengeance\"><em>Wallace &amp; Gromit</em></a>, <a href=\"https://www.theverge.com/2022/1/20/22893091/netflix-chicken-run-sequel-dawn-of-the-nuggets\"><em>Chicken Run</em></a>, and <em>Shaun the Sheep</em>, and <em>Chicken Run </em>franchises, is working with The Pokémon Company International on a mysterious new project.</p>\n<p id=\"npGBEE\">In a surprising turn of events, TCPi and Aardman announced today that they’re teaming up for “a special project” that’s set to be released some time in 2027. In <a href=\"https://press.pokemon.com/en/releases/Pokemon-Aardman-Team-Up-for-Exciting-Collaboration-Coming-2027\">a press release</a> about the collaboration, TCP marketing and media VP Taito Okiura described it as “a dream partnership for Pokémon.” Aardman’s managing director Sean Clarke added that it was both a huge honor and privilege to be tasked with presenting the Pokémon world in a new way.</p>\n<div id=\"u5fPoP\">\n<blockquote class=\"twitter-tweet\" data-dnt=\"true\" align=\"center\">\n<p lang=\"fr\" dir=\"ltr\">Pokémon × <a href=\"https://twitter.com/aardman?ref_src=twsrc%5Etfw\">@aardman</a><br>Coming in 2027! <a href=\"https://t.co/DQPbtekKXo\">pic.twitter.com/DQPbtekKXo</a></p>— Pokémon (@Pokemon) <a href=\"https://twitter.com/Pokemon/status/1866845745313136673?ref_src=twsrc%5Etfw\">December 11, 2024</a>\n</blockquote>\n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n</div>\n<p id=\"qmLNQD\">“Bringing together Pokémon, the world’s biggest entertainment brand, together with our love of craft, character and comedic storytelling feels incredibly exciting,” Clarke said. “Aardman and TPCi share an emphasis on heritage and attention to detail as well as putting our fans and audiences at the heart of what we do, which we know will steer us right as we together create charming, original and new stories for audiences around the world.”</p>\n<p id=\"U2vKWw\">Aside from the projected release year, there aren’t all that many details about what the collaboration is or how we’ll be able to consume it. Clearly, stop-motion claymation will be involved, but what’s less obvious is whether this will end up being a movie, a series, or perhaps a game — which <a href=\"https://www.youtube.com/watch?v=iYJuoDhPDfE\">wouldn’t be a first for Aardman</a>. This sort of team-up makes a lot of sense for TCPi after the success of <a href=\"https://www.theverge.com/2019/5/2/18525323/detective-pikachu-review-pokemon\"><em>Detective Pikachu</em></a><em> </em>and surprise delights like <a href=\"https://www.theverge.com/24017505/pokemon-concierge-review-netflix\"><em>Pokémon Concierge</em></a><em>. </em>And the closer we get to 2027, it’s feels like there’s a very strong chance this will be something that has people buzzing.</p>\n\n",
    "summary": "TCPi / Aardman\n    \n\n  \nAardman Animation, the studio behind the Wallace & Gromit, Chicken Run, and Shaun the Sheep, and Chicken Run franchises, is working with The Pokémon Company International on a mysterious new project.\nIn a surprising turn of events, TCPi and Aardman announced today that they’re teaming up for “a special project” that’s set to be released some time in 2027. In a press release about the collaboration, TCP marketing and media VP Taito Okiura described it as “a dream partnership for Pokémon.” Aardman’s managing director Sean Clarke added that it was both a huge honor and privilege to be tasked with presenting the Pokémon world in a new way.\nPokémon × @aardman\nComing in 2027! pic.twitter.com/DQPbtekKXo\n— Pokémon (@Pokemon) December 11, 2024\n\n\n\n\n“Bringing together Pokémon, the world’s biggest entertainment brand, together with our love of craft, character and comedic storytelling feels incredibly exciting,” Clarke said. “Aardman and TPCi share an emphasis on heritage and attention to detail as well as putting our fans and audiences at the heart of what we do, which we know will steer us right as we together create charming, original and new stories for audiences around the world.”\nAside from the projected release year, there aren’t all that many details about what the collaboration is or how we’ll be able to consume it. Clearly, stop-motion claymation will be involved, but what’s less obvious is whether this will end up being a movie, a series, or perhaps a game — which wouldn’t be a first for Aardman. This sort of team-up makes a lot of sense for TCPi after the success of Detective Pikachu and surprise delights like Pokémon Concierge. And the closer we get to 2027, it’s feels like there’s a very strong chance this will be something that has people buzzing.",
    "author": "Charles Pulliam-Moore"
  },
  {
    "title": "Why every company wants a podcast now",
    "link": "https://www.theverge.com/24318644/podcast-election-vc-marketing-business-decoder-interview",
    "pubDate": "2024-12-11T15:00:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"Collage of podcaster\" src=\"https://cdn.vox-cdn.com/thumbor/ckVa9LkA7zbPPOYR5rjfm7axtLk=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780268/DCD_1212_Podcast.0.jpg\" />\n        <figcaption>Image: The Verge. Photos: Getty</figcaption>\n    </figure>\n\n  <p id=\"GqNY2Q\">Hello, and welcome to <em>Decoder</em>! I’m David Pierce, editor-at-large of <em>The Verge</em>. As you may have noticed, we’re dropping some extra episodes in the feed this week. You’ll have Nilay back on Friday and for next week, as we run toward the end of the year.</p>\n<p id=\"ZltfB6\">But I’m really excited to be here with you all today because I’m getting to talk about one of my favorite things: podcasts. There’s something strange happening these days in the podcast world — well, actually, there are kind of a lot of things happening. It’s <a href=\"https://www.vulture.com/article/kamala-harris-should-do-joe-rogans-podcast.html\">been a wild year</a>.</p>\n<div id=\"pDN0AV\"></div>\n<p id=\"RCywXt\">One thing I’ve noticed recently is the way companies that deal in money have been using podcasts not just as an entertainment medium but also as a weird hybrid of marketing, thought leadership, and networking. It’s something we’ve seen for a few years now with venture capital firms, for example: not only do most of the top-level VC companies have their own podcasts but also people who do podcasts <em>about</em> venture capital <a href=\"https://www.geekwire.com/2024/ben-gilbert-and-david-rosenthal-founders-of-podcast-acquired-are-raising-an-investment-fund/\">end up going into it</a> after <a href=\"https://sifted.eu/articles/harry-stebbings-third-fund-400-news\">meeting and talking to all these folks</a>.</p>\n<p id=\"SvGKVx\">It’s kind of a weird, complicated web that goes both ways, and it’s not getting any less weird or less complicated once you add stuff like crypto and politics to the mix. So I...</p>\n  <p><a href=\"https://www.theverge.com/24318644/podcast-election-vc-marketing-business-decoder-interview\">Read the full story at The Verge.</a></p>\n\n",
    "summary": "Image: The Verge. Photos: Getty\n    \n\n  \nHello, and welcome to Decoder! I’m David Pierce, editor-at-large of The Verge. As you may have noticed, we’re dropping some extra episodes in the feed this week. You’ll have Nilay back on Friday and for next week, as we run toward the end of the year.\nBut I’m really excited to be here with you all today because I’m getting to talk about one of my favorite things: podcasts. There’s something strange happening these days in the podcast world — well, actually, there are kind of a lot of things happening. It’s been a wild year.\n\nOne thing I’ve noticed recently is the way companies that deal in money have been using podcasts not just as an entertainment medium but also as a weird hybrid of marketing, thought leadership, and networking. It’s something we’ve seen for a few years now with venture capital firms, for example: not only do most of the top-level VC companies have their own podcasts but also people who do podcasts about venture capital end up going into it after meeting and talking to all these folks.\nIt’s kind of a weird, complicated web that goes both ways, and it’s not getting any less weird or less complicated once you add stuff like crypto and politics to the mix. So I...\nRead the full story at The Verge.",
    "author": "David Pierce"
  },
  {
    "title": "We asked our staff for their 2025 predictions",
    "link": "https://www.theverge.com/24305936/2025-tech-predictions-ai-tiktok-ban-smart-home-fda-pcs",
    "pubDate": "2024-12-11T15:00:00.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"\" src=\"https://cdn.vox-cdn.com/thumbor/1xDe_eMFBtXu_NCEA6dtTmnWQTI=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780267/VRG_TEC_248_Illo.0.jpg\" />\n    </figure>\n\n  <p id=\"fSajhU\">2024 had some stand-out moments in tech: from <a href=\"https://www.theverge.com/2024/8/22/24225972/ai-photo-era-what-is-reality-google-pixel-9\">AI-generated images</a> to TikTok’s <a href=\"https://www.theverge.com/2024/12/6/24297454/tiktok-divest-or-ban-dc-circuit-court-appeals\">near ban,</a> to <a href=\"https://www.theverge.com/24316196/pantone-2025-color-of-the-year-mocha-mousse-party\">beigecore</a>, <a href=\"https://www.theverge.com/2024/7/19/24201864/crowdstrike-outage-explained-microsoft-windows-bsod\">Windows BSOD</a>, the <a href=\"https://www.theverge.com/2024/10/1/24259284/oura-ring-samsung-galaxy-ring-ultrahuman-ring-air-ringconn-circular-ring-evie-ring-review-wearables\">best smart ring</a> of them all, and the list goes on. If you want to see the technology that we just couldn’t ignore, check out our video <a href=\"https://youtu.be/yYjBf94WY2M?si=0Es9N-8MDBPSzduJ\">here</a>. </p>\n<p id=\"SXX5RC\">But now, we’re looking toward 2025, and it’s gearing up to be another eventful year. Will we have an actual fulfilling X replacement? Will more health and wellness features be cleared by the FDA on wearables? Will nothing really change but everything will just get more expensive? While we’re not fortune tellers, we can probably take some educated guesses about what’s to come. </p>\n<p id=\"FmGMVt\">We asked <em>Verge</em> staff for their biggest predictions on trends we could see in 2025. Take a look, and spoiler alert, some are already coming true.</p>\n<p id=\"ruJCP5\"></p>\n<p id=\"4GI8el\"></p>\n  <p><a href=\"https://www.theverge.com/24305936/2025-tech-predictions-ai-tiktok-ban-smart-home-fda-pcs\">Read the full story at The Verge.</a></p>\n\n",
    "summary": "2024 had some stand-out moments in tech: from AI-generated images to TikTok’s near ban, to beigecore, Windows BSOD, the best smart ring of them all, and the list goes on. If you want to see the technology that we just couldn’t ignore, check out our video here. \nBut now, we’re looking toward 2025, and it’s gearing up to be another eventful year. Will we have an actual fulfilling X replacement? Will more health and wellness features be cleared by the FDA on wearables? Will nothing really change but everything will just get more expensive? While we’re not fortune tellers, we can probably take some educated guesses about what’s to come. \nWe asked Verge staff for their biggest predictions on trends we could see in 2025. Take a look, and spoiler alert, some are already coming true.\n\n\nRead the full story at The Verge.",
    "author": "Verge Staff"
  },
  {
    "title": "YouTube is a hit on TVs — and is starting to act like it",
    "link": "https://www.theverge.com/2024/12/11/24318434/youtube-living-room-tv-growth-2024",
    "pubDate": "2024-12-11T14:05:53.000Z",
    "content": "  \n\n    <figure>\n      <img alt=\"YouTube’s logo with geometric design in the background\" src=\"https://cdn.vox-cdn.com/thumbor/ubayt-UIcdhmZNpmPNForQOusQ4=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/73780210/acastro_STK092_04.0.jpg\" />\n        <figcaption>Illustration by Alex Castro / The Verge</figcaption>\n    </figure>\n\n  <p id=\"7qKXs1\">YouTube just released some new stats that show how the service is being consumed on televisions, and the numbers are enormous. Watch time on TV for sports content was up 30 percent year over year; viewers watched more than 400 million hours of podcasts on their TVs every month. </p>\n<p id=\"FrnPcF\">This is YouTube we’re talking about, though, so of course the numbers are huge. The living room has been YouTube’s fastest-growing platform for years — Alphabet’s chief business officer, Philipp Schindler, said on the company’s <a href=\"https://abc.xyz/2024-q3-earnings-call/\">most recent earnings call</a> that watch time is growing across YouTube “with particular strength in Shorts and in the living room.” Even as YouTube continues to dominate basically all facets of the entertainment business, the arrow on your TV still points up.</p>\n<p id=\"I0psR5\">The trend hasn’t changed in forever, but YouTube has spent the last couple of years finally doing something about it. It launched a way to <a href=\"https://www.theverge.com/2022/6/1/23149976/youtube-connect-tv-phone-android-ios\">sync your phone and your TV</a>, so you can watch a video on the big screen and interact with it on the small one. Earlier this year, the company <a href=\"https://www.theverge.com/2024/3/13/24099533/youtube-tv-app-redesign-comments-shopping\">redesigned the TV interface</a> to make it easier to find comments, links, and channel pages while you’re watching a video. It <a href=\"https://www.theverge.com/2024/9/18/24248206/youtube-season-episode-format-organizing-tv-netflix-binge\">redesigned those channel...</a></p>\n  <p><a href=\"https://www.theverge.com/2024/12/11/24318434/youtube-living-room-tv-growth-2024\">Read the full story at The Verge.</a></p>\n\n",
    "summary": "Illustration by Alex Castro / The Verge\n    \n\n  \nYouTube just released some new stats that show how the service is being consumed on televisions, and the numbers are enormous. Watch time on TV for sports content was up 30 percent year over year; viewers watched more than 400 million hours of podcasts on their TVs every month. \nThis is YouTube we’re talking about, though, so of course the numbers are huge. The living room has been YouTube’s fastest-growing platform for years — Alphabet’s chief business officer, Philipp Schindler, said on the company’s most recent earnings call that watch time is growing across YouTube “with particular strength in Shorts and in the living room.” Even as YouTube continues to dominate basically all facets of the entertainment business, the arrow on your TV still points up.\nThe trend hasn’t changed in forever, but YouTube has spent the last couple of years finally doing something about it. It launched a way to sync your phone and your TV, so you can watch a video on the big screen and interact with it on the small one. Earlier this year, the company redesigned the TV interface to make it easier to find comments, links, and channel pages while you’re watching a video. It redesigned those channel...\nRead the full story at The Verge.",
    "author": "David Pierce"
  }
]